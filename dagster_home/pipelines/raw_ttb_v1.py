"""
Auto-generated Dagster pipeline for raw_ttb
Generated by Inspector Dom
Entity Type: INTERIM
"""

from dagster import (
    asset,
    AssetExecutionContext,
    MaterializeResult,
    MetadataValue,
    RetryPolicy,
)
from typing import Dict, Any, List, Optional
import logging
import traceback
from datetime import datetime

# Configure logging
logger = logging.getLogger(__name__)

# ============================================================================
# EXTRACTION ASSETS
# ============================================================================

@asset(
    name="extract_raw_ttb",
    description="Extract data from source artifacts using template",
    compute_kind="extraction",
    retry_policy=RetryPolicy(max_retries=3),
)
def extract_raw_ttb(context: AssetExecutionContext) -> Dict[str, Any]:
    """
    Extract raw_ttb data from source artifacts.

    Strategy: template
    Entity Type: INTERIM
    """
    try:
        context.log.info(f"Starting extraction for raw_ttb")

        # Initialize Supabase client
        from supabase import create_client
        import os

        supabase = create_client(
            os.getenv("NEXT_PUBLIC_SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        )

        # Fetch source artifacts
        source_ids = ["753349ee-92cf-4ea1-8859-0a97e32b642f"]

        query = supabase.table("artifacts").select("*")
        if source_ids:
            query = query.in_("source_id", source_ids)

        artifacts_response = query.execute()
        artifacts = artifacts_response.data

        context.log.info(f"Found {len(artifacts)} artifacts to process")

        # Extract data from each artifact
        extracted_records = []
        failed_count = 0

        for artifact in artifacts:
            try:
                record = extract_from_artifact(
                    artifact,
                    template="98b382f1-b1f3-41ec-8ba6-372af573c7bc",
                    context=context
                )

                if record:
                    # Add metadata
                    record["_extraction_timestamp"] = datetime.utcnow().isoformat()
                    record["_source_artifact_id"] = artifact["id"]
                    record["_source_filename"] = artifact.get("original_filename")

                    # Validate required fields
                    if validate_record(record, ["ttbid"]):
                        extracted_records.append(record)
                    else:
                        context.log.warning(f"Validation failed for artifact {artifact['id']}")
                        failed_count += 1
                else:
                    failed_count += 1

            except Exception as e:
                context.log.error(f"Failed to extract artifact {artifact['id']}: {str(e)}")
                failed_count += 1
                continue

        success_rate = (len(extracted_records) / len(artifacts) * 100) if artifacts else 0

        context.log.info(
            f"Extraction complete: {len(extracted_records)} successful, "
            f"{failed_count} failed ({success_rate:.1f}% success rate)"
        )

        # Quality check: Alert if success rate is too low
        if success_rate < 80:
            context.log.warning(
                f"⚠️ Low extraction success rate: {success_rate:.1f}%. "
                f"Template may need adjustment."
            )

        return {
            "records": extracted_records,
            "metadata": {
                "total_artifacts": len(artifacts),
                "extracted": len(extracted_records),
                "failed": failed_count,
                "success_rate": success_rate,
            }
        }

    except Exception as e:
        context.log.error(f"Extraction failed: {str(e)}")
        context.log.error(traceback.format_exc())
        raise


def extract_from_artifact(
    artifact: Dict[str, Any],
    template: Optional[str],
    context: AssetExecutionContext
) -> Optional[Dict[str, Any]]:
    """
    Extract data from a single artifact using template or AI.
    """
    try:
        raw_content = artifact.get("raw_content", {})

        # Priority 1: AI-extracted fields
        if isinstance(raw_content, dict) and "fields" in raw_content:
            context.log.debug(f"Using AI-extracted fields for {artifact['id']}")
            return raw_content["fields"]

        # Priority 2: Template extraction from HTML text
        if isinstance(raw_content, dict) and "text" in raw_content:
            context.log.debug(f"Extracting from HTML text for {artifact['id']}")
            text = raw_content["text"]

            # Field extraction patterns
            extracted = {}
                        # Extract ttbid
            match = re.search(r'TTB\s+ID\s*(\d+)', text, re.IGNORECASE)
            if match:
                extracted["ttbid"] = match.group(1).strip()
            # Extract ct
            match = re.search(r'\bCT\s*(\d+)', text, re.IGNORECASE)
            if match:
                extracted["ct"] = match.group(1).strip()
            # Extract or
            match = re.search(r'\bOR\s*(\d+)', text, re.IGNORECASE)
            if match:
                extracted["or"] = match.group(1).strip()
            # Extract productsource
            match = re.search(r'productsource[:\s]+([^\n]+)', text, re.IGNORECASE)
            if match:
                extracted["productsource"] = match.group(1).strip()
            # Extract producttype
            match = re.search(r'producttype[:\s]+([^\n]+)', text, re.IGNORECASE)
            if match:
                extracted["producttype"] = match.group(1).strip()

            return extracted if extracted else None

        # Priority 3: Use metadata
        if artifact.get("metadata", {}).get("extracted_data"):
            context.log.debug(f"Using metadata extraction for {artifact['id']}")
            return artifact["metadata"]["extracted_data"]

        return None

    except Exception as e:
        context.log.error(f"Extraction error for artifact {artifact['id']}: {str(e)}")
        return None


# ============================================================================
# TRANSFORMATION ASSETS
# ============================================================================

# No transformation needed for INTERIM entities
# Data flows directly to downstream Reference/Master entities


# ============================================================================
# LOAD ASSETS
# ============================================================================

@asset(
    name="load_raw_ttb",
    description="Load transformed data into raw_ttb table",
    compute_kind="load",
    deps=["extract_raw_ttb"],
    retry_policy=RetryPolicy(max_retries=3),
)
def load_raw_ttb(
    context: AssetExecutionContext,
    extract_raw_ttb: Dict[str, Any]
) -> MaterializeResult:
    """
    Load raw_ttb data into database.

    Target Table: raw_ttb
    Entity Type: INTERIM
    """
    try:
        context.log.info(f"Starting load for raw_ttb")

        # Initialize Supabase client
        from supabase import create_client
        import os

        supabase = create_client(
            os.getenv("NEXT_PUBLIC_SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        )

        records = extract_raw_ttb["records"]

        if not records:
            context.log.warning("No records to load")
            return MaterializeResult(
                metadata={
                    "records_loaded": 0,
                    "records_failed": 0,
                }
            )

        # Batch insert with error handling
        batch_size = 100
        loaded_count = 0
        failed_count = 0

        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]

            try:
                response = supabase.table("raw_ttb").insert(batch).execute()
                loaded_count += len(batch)
                context.log.info(f"Loaded batch {i//batch_size + 1}: {len(batch)} records")

            except Exception as e:
                context.log.error(f"Batch insert failed: {str(e)}")

                # Try inserting records one by one
                for record in batch:
                    try:
                        supabase.table("raw_ttb").insert(record).execute()
                        loaded_count += 1
                    except Exception as record_error:
                        context.log.error(
                            f"Failed to insert record: {record.get('_source_filename', 'unknown')}"
                        )
                        failed_count += 1

        success_rate = (loaded_count / len(records) * 100) if records else 0

        context.log.info(
            f"Load complete: {loaded_count} loaded, {failed_count} failed "
            f"({success_rate:.1f}% success rate)"
        )

        # Quality check
        if success_rate < 95:
            context.log.warning(
                f"⚠️ Load success rate below 95%: {success_rate:.1f}%"
            )

        return MaterializeResult(
            metadata={
                "records_loaded": MetadataValue.int(loaded_count),
                "records_failed": MetadataValue.int(failed_count),
                "success_rate": MetadataValue.float(success_rate),
                "table_name": MetadataValue.text("raw_ttb"),
            }
        )

    except Exception as e:
        context.log.error(f"Load failed: {str(e)}")
        context.log.error(traceback.format_exc())
        raise


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def validate_record(record: Dict[str, Any], required_fields: List[str]) -> bool:
    """Validate that all required fields are present and non-empty."""
    for field in required_fields:
        if field not in record or record[field] is None or record[field] == "":
            return False
    return True


def deduplicate_records(
    records: List[Dict[str, Any]],
    key_fields: List[str]
) -> List[Dict[str, Any]]:
    """Remove duplicate records based on key fields."""
    seen = set()
    unique_records = []

    for record in records:
        # Create a key from specified fields
        key = tuple(record.get(field) for field in key_fields)

        if key not in seen:
            seen.add(key)
            unique_records.append(record)

    return unique_records


def parse_date(value: Any) -> Optional[str]:
    """Parse various date formats to ISO format."""
    if not value:
        return None

    import dateutil.parser

    try:
        if isinstance(value, str):
            dt = dateutil.parser.parse(value)
            return dt.date().isoformat()
        return str(value)
    except Exception:
        return None


def parse_timestamp(value: Any) -> Optional[str]:
    """Parse various timestamp formats to ISO format."""
    if not value:
        return None

    import dateutil.parser

    try:
        if isinstance(value, str):
            dt = dateutil.parser.parse(value)
            return dt.isoformat()
        return str(value)
    except Exception:
        return None

