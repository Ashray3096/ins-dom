"""
Auto-generated Dagster pipeline for raw_nabca_table_6
Generated by Inspector Dom
Entity Type: INTERIM
"""

from dagster import (
    asset,
    AssetExecutionContext,
    MaterializeResult,
    MetadataValue,
    RetryPolicy,
)
from typing import Dict, Any, List, Optional
import logging
import traceback
import re
from datetime import datetime

import os
import boto3
import io
from difflib import SequenceMatcher
from PyPDF2 import PdfReader, PdfWriter
from supabase import create_client

# Configure logging
logger = logging.getLogger(__name__)

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def parse_report_date_from_filename(filename: str) -> tuple:
    """
    Parse month and year from NABCA filename format.
    Expected format: XXX_XXX_MMYY.PDF (e.g., 631_9L_0125.PDF = January 2025)

    Returns:
        tuple: (month_name, year) or (None, None) if parsing fails
    """
    try:
        # Extract 4-digit pattern that looks like MMYY
        match = re.search(r'_(\d{4})\.', filename)
        if match:
            mmyy = match.group(1)
            month_num = int(mmyy[:2])
            year_suffix = int(mmyy[2:])

            # Convert to full year (20YY format)
            full_year = f"20{year_suffix:02d}"

            # Convert month number to name
            month_names = ['January', 'February', 'March', 'April', 'May', 'June',
                          'July', 'August', 'September', 'October', 'November', 'December']

            if 1 <= month_num <= 12:
                month_name = month_names[month_num - 1]
                return (month_name, full_year)

        # Fallback: Try to find any 4-digit pattern
        match = re.search(r'(\d{4})', filename)
        if match:
            mmyy = match.group(1)
            month_num = int(mmyy[:2])
            year_suffix = int(mmyy[2:])

            if 1 <= month_num <= 12:
                full_year = f"20{year_suffix:02d}"
                month_names = ['January', 'February', 'March', 'April', 'May', 'June',
                              'July', 'August', 'September', 'October', 'November', 'December']
                month_name = month_names[month_num - 1]
                return (month_name, full_year)

        return (None, None)
    except Exception as e:
        logger.warning(f"Failed to parse date from filename '{filename}': {e}")
        return (None, None)

# ============================================================================
# EXTRACTION ASSETS
# ============================================================================

@asset(
    name="extract_raw_nabca_table_6",
    description="Extract Vendor Top 100 data from NABCA PDFs using AWS Textract",
    compute_kind="extraction:textract",
    retry_policy=RetryPolicy(max_retries=3),
)
def extract_raw_nabca_table_6(context: AssetExecutionContext) -> Dict[str, Any]:
    """
    Extract raw_nabca_table_6 data from NABCA PDF artifacts.

    NABCA Section: Vendor Top 100
    Page Range: 365-366
    Extraction Method: AWS Textract (Table Detection)
    Entity Type: INTERIM
    """
    try:
        context.log.info(f"Starting NABCA extraction for raw_nabca_table_6")
        context.log.info(f"Section: Vendor Top 100, Pages: 365-366")

        # Initialize clients (imports are at module level)
        supabase = create_client(
            os.getenv("NEXT_PUBLIC_SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        )

        # AWS clients
        textract_client = boto3.client('textract', region_name=os.getenv("AWS_REGION", "us-east-1"))
        s3_client = boto3.client('s3', region_name=os.getenv("AWS_REGION", "us-east-1"))

        # Fetch PDF artifacts
        source_ids = ["cc74c14b-f43c-4b76-8c2d-b78f901989bb"]

        query = supabase.table("artifacts").select("*").eq("artifact_type", "pdf")
        if source_ids:
            query = query.in_("source_id", source_ids)

        artifacts_response = query.execute()
        artifacts = artifacts_response.data

        context.log.info(f"Found {len(artifacts)} PDF artifacts to process")

        # Field mapping (semantic field names)
        semantic_fields = [{"name":"vendor","displayName":"vendor","dataType":"TEXT"},{"name":"rank","displayName":"rank","dataType":"NUMBER"},{"name":"share_of_market","displayName":"share_of_market","dataType":"NUMBER"},{"name":"l12m_this_year","displayName":"l12m_this_year","dataType":"NUMBER"},{"name":"l12m_prior_year","displayName":"l12m_prior_year","dataType":"NUMBER"},{"name":"l12m_change","displayName":"l12m_change","dataType":"NUMBER"},{"name":"ytd_this_year","displayName":"ytd_this_year","dataType":"NUMBER"},{"name":"ytd_last_year","displayName":"ytd_last_year","dataType":"NUMBER"},{"name":"ytd_change","displayName":"ytd_change","dataType":"NUMBER"},{"name":"current_month_this_year","displayName":"current_month_this_year","dataType":"NUMBER"},{"name":"current_month_last_year","displayName":"current_month_last_year","dataType":"NUMBER"},{"name":"current_month_change","displayName":"current_month_change","dataType":"NUMBER"}]

        # Extract data from each PDF
        extracted_records = []
        failed_count = 0

        for artifact in artifacts:
            try:
                context.log.info(f"Processing artifact: {artifact['id']}")

                # Get PDF file from S3 or Supabase
                pdf_data = get_artifact_pdf(supabase, s3_client, artifact, context)
                if not pdf_data:
                    context.log.error(f"Failed to retrieve PDF for artifact {artifact['id']}")
                    failed_count += 1
                    continue

                # Extract specific page range (365-366)
                context.log.info(f"Extracting pages 365-366...")
                section_pdf_bytes = extract_pdf_page_range(
                    pdf_data,
                    365,
                    366,
                    context
                )

                # Upload section to S3 for Textract processing
                s3_bucket = os.getenv("TEXTRACT_S3_BUCKET") or os.getenv("AWS_S3_BUCKET")
                s3_key = f"textract-temp/nabca-extraction/{artifact['id']}/section.pdf"

                context.log.info(f"Uploading to S3: s3://{s3_bucket}/{s3_key}")
                s3_client.put_object(
                    Bucket=s3_bucket,
                    Key=s3_key,
                    Body=section_pdf_bytes
                )

                # Start Textract analysis
                context.log.info("Starting Textract async analysis...")
                textract_response = textract_client.start_document_analysis(
                    DocumentLocation={'S3Object': {'Bucket': s3_bucket, 'Name': s3_key}},
                    FeatureTypes=['TABLES', 'FORMS']
                )

                job_id = textract_response['JobId']
                context.log.info(f"Textract job started: {job_id}")

                # Poll for completion
                import time
                max_wait = 600  # 10 minutes
                wait_interval = 5
                elapsed = 0

                while elapsed < max_wait:
                    time.sleep(wait_interval)
                    elapsed += wait_interval

                    status_response = textract_client.get_document_analysis(JobId=job_id)
                    status = status_response['JobStatus']

                    if status == 'SUCCEEDED':
                        context.log.info(f"Textract job completed after {elapsed}s")
                        break
                    elif status == 'FAILED':
                        raise Exception(f"Textract job failed: {status_response.get('StatusMessage')}")

                    context.log.debug(f"Textract job status: {status} (waited {elapsed}s)")

                if status != 'SUCCEEDED':
                    raise Exception(f"Textract job timed out after {max_wait}s")

                # Get all pages of results - MUST collect ALL blocks first!
                # Table blocks reference cell/word blocks that might be on different pages
                context.log.info("Retrieving all Textract blocks...")
                all_blocks = status_response.get('Blocks', [])
                next_token = status_response.get('NextToken')

                while next_token:
                    response = textract_client.get_document_analysis(JobId=job_id, NextToken=next_token)
                    all_blocks.extend(response.get('Blocks', []))
                    next_token = response.get('NextToken')

                context.log.info(f"Retrieved {len(all_blocks)} blocks from Textract")

                # Parse tables from ALL blocks at once
                tables = parse_textract_tables(all_blocks)
                extraction_result = {'tables': tables, 'keyValuePairs': []}

                context.log.info(f"Textract complete: {len(extraction_result['tables'])} tables detected")

                # Extract table data and map to semantic fields
                records = extract_nabca_table_data(
                    extraction_result,
                    semantic_fields,
                    context
                )

                # Parse report month and year from filename
                filename = artifact.get("original_filename", "")
                report_month, report_year = parse_report_date_from_filename(filename)

                if report_month and report_year:
                    context.log.info(f"ðŸ“… Parsed date from '{filename}': {report_month} {report_year}")
                else:
                    context.log.warning(f"âš ï¸  Could not parse date from filename: {filename}")

                # Add artifact metadata to each record
                for record in records:
                    record["_artifact_id"] = artifact["id"]
                    record["_source_id"] = artifact.get("source_id")
                    record["_extracted_at"] = datetime.utcnow().isoformat()

                    # Add parsed report month and year
                    record["report_month"] = report_month
                    record["report_year"] = report_year

                extracted_records.extend(records)
                context.log.info(f"Extracted {len(records)} records from artifact {artifact['id']}")

            except Exception as e:
                context.log.error(f"Failed to process artifact {artifact['id']}: {str(e)}")
                context.log.error(traceback.format_exc())
                failed_count += 1
                continue

        success_rate = (len(extracted_records) / (len(extracted_records) + failed_count) * 100) if (len(extracted_records) + failed_count) > 0 else 0

        context.log.info(
            f"NABCA extraction complete: {len(extracted_records)} records from {len(artifacts)} artifacts, "
            f"{failed_count} failed ({success_rate:.1f}% success rate)"
        )

        # Quality check
        if success_rate < 80:
            context.log.warning(
                f"âš ï¸ Low extraction success rate: {success_rate:.1f}%. "
                f"Check Textract results or PDF quality."
            )

        return {
            "records": extracted_records,
            "metadata": {
                "total_artifacts": len(artifacts),
                "extracted_records": len(extracted_records),
                "failed_artifacts": failed_count,
                "success_rate": success_rate,
                "nabca_section": "Vendor Top 100",
                "page_range": "365-366",
            }
        }

    except Exception as e:
        context.log.error(f"NABCA extraction failed: {str(e)}")
        context.log.error(traceback.format_exc())
        raise


def get_artifact_pdf(supabase, s3_client, artifact: Dict[str, Any], context) -> bytes:
    """Retrieve PDF file data from S3 or Supabase storage."""
    try:
        # Check if artifact has S3 metadata
        if artifact.get("metadata", {}).get("s3_bucket") and artifact.get("metadata", {}).get("s3_key"):
            context.log.info(f"Downloading from S3: s3://{artifact['metadata']['s3_bucket']}/{artifact['metadata']['s3_key']}")
            response = s3_client.get_object(
                Bucket=artifact["metadata"]["s3_bucket"],
                Key=artifact["metadata"]["s3_key"]
            )
            return response['Body'].read()
        else:
            # Download from Supabase storage
            file_path = artifact.get("file_path")
            if not file_path:
                raise Exception("No file_path found in artifact")

            context.log.info(f"Downloading from Supabase storage: {file_path}")
            response = supabase.storage.from_("artifacts").download(file_path)
            return response

    except Exception as e:
        context.log.error(f"Failed to retrieve PDF: {str(e)}")
        return None


def extract_pdf_page_range(pdf_data: bytes, start_page: int, end_page: int, context) -> bytes:
    """Extract specific page range from PDF (1-indexed)."""
    try:
        # Read PDF
        pdf_reader = PdfReader(io.BytesIO(pdf_data))
        total_pages = len(pdf_reader.pages)

        context.log.info(f"PDF has {total_pages} pages, extracting pages {start_page}-{end_page}")

        # Validate page range
        if start_page < 1 or end_page > total_pages:
            raise Exception(f"Invalid page range {start_page}-{end_page} for PDF with {total_pages} pages")

        # Create new PDF with selected pages
        pdf_writer = PdfWriter()
        for page_num in range(start_page - 1, end_page):  # Convert to 0-indexed
            pdf_writer.add_page(pdf_reader.pages[page_num])

        # Write to bytes
        output_buffer = io.BytesIO()
        pdf_writer.write(output_buffer)
        output_buffer.seek(0)

        return output_buffer.read()

    except Exception as e:
        context.log.error(f"Failed to extract PDF pages: {str(e)}")
        raise


def parse_textract_tables(blocks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Parse Textract blocks into table structure."""
    tables = []

    # Find all TABLE blocks
    table_blocks = [b for b in blocks if b.get('BlockType') == 'TABLE']

    for table_block in table_blocks:
        # Find all CELL blocks for this table
        cell_blocks = []
        if 'Relationships' in table_block:
            for rel in table_block['Relationships']:
                if rel['Type'] == 'CHILD':
                    for cell_id in rel['Ids']:
                        cell_block = next((b for b in blocks if b.get('Id') == cell_id), None)
                        if cell_block and cell_block.get('BlockType') == 'CELL':
                            cell_blocks.append(cell_block)

        # Build table grid
        if not cell_blocks:
            continue

        max_row = max(c.get('RowIndex', 0) for c in cell_blocks)
        max_col = max(c.get('ColumnIndex', 0) for c in cell_blocks)

        # Initialize grid
        grid = [['' for _ in range(max_col)] for _ in range(max_row)]

        # Fill grid
        for cell in cell_blocks:
            row = cell.get('RowIndex', 1) - 1  # Convert to 0-indexed
            col = cell.get('ColumnIndex', 1) - 1

            # Get cell text
            cell_text = ''
            if 'Relationships' in cell:
                for rel in cell['Relationships']:
                    if rel['Type'] == 'CHILD':
                        for word_id in rel['Ids']:
                            word_block = next((b for b in blocks if b.get('Id') == word_id), None)
                            if word_block and word_block.get('BlockType') == 'WORD':
                                cell_text += word_block.get('Text', '') + ' '

            grid[row][col] = cell_text.strip()

        tables.append({'data': grid})

    return tables


def extract_nabca_table_data(
    textract_result: Dict[str, Any],
    semantic_fields: List[Dict[str, str]],
    context: AssetExecutionContext
) -> List[Dict[str, Any]]:
    """
    Extract table data from Textract results and map to semantic field names.

    Uses fuzzy matching to map Textract column headers to semantic field names.
    Handles multi-table NABCA PDFs that span multiple pages.
    """
    from difflib import SequenceMatcher

    tables = textract_result.get("tables", [])
    if not tables:
        context.log.warning("No tables found in Textract results")
        return []

    context.log.info(f"Processing {len(tables)} tables...")

    # Merge all tables (NABCA PDFs often span multiple pages/tables)
    all_table_data = []
    for idx, table in enumerate(tables):
        table_data_raw = table.get("data", [])
        context.log.info(f"  Table {idx + 1}: {len(table_data_raw)} rows")

        # For first table, include everything
        if idx == 0:
            all_table_data.extend(table_data_raw)
        else:
            # For subsequent tables, skip header rows and merge data
            # Detect header by finding row with most non-empty cells
            header_idx = 0
            max_non_empty = 0
            for row_idx, row in enumerate(table_data_raw[:5]):
                non_empty = sum(1 for cell in row if cell and cell.strip())
                if non_empty > max_non_empty:
                    max_non_empty = non_empty
                    header_idx = row_idx

            # Skip header and metadata rows, only take data
            data_start_idx = header_idx + 1
            context.log.info(f"    Skipping {data_start_idx} header rows, merging {len(table_data_raw) - data_start_idx} data rows")
            all_table_data.extend(table_data_raw[data_start_idx:])

    table_data = all_table_data
    context.log.info(f"Merged table data: {len(table_data)} total rows")

    if len(table_data) < 2:
        context.log.warning("Table has no data rows")
        return []

    # Show first 10 rows of raw data for debugging
    context.log.info(f"ðŸ“Š RAW TABLE DATA ({len(table_data)} rows total):")
    for idx, row in enumerate(table_data[:10]):
        context.log.info(f"  Row {idx}: {row[:3]}... (showing first 3 cells)")

    # Find the real header row - EXACT LOGIC FROM WORKING TEST SCRIPT
    header_row_idx = 0
    max_non_empty = 0

    for idx, row in enumerate(table_data[:5]):
        non_empty_count = sum(1 for cell in row if cell and cell.strip() and cell.strip() != "'")
        if non_empty_count > max_non_empty:
            max_non_empty = non_empty_count
            header_row_idx = idx
        context.log.info(f"  Row {idx}: {non_empty_count} non-empty cells")

    context.log.info(f"âœ… Detected header row at index {header_row_idx} (has {max_non_empty} non-empty cells)")

    headers = table_data[header_row_idx]
    data_rows = table_data[header_row_idx + 1:]

    context.log.info(f"ðŸ“‹ Headers: {headers}")
    context.log.info(f"ðŸ“ˆ Data rows: {len(data_rows)}")

    context.log.info(f"Table has {len(headers)} columns and {len(data_rows)} data rows")
    context.log.info(f"ðŸ“‹ Textract Headers: {headers}")
    context.log.info(f"ðŸŽ¯ Semantic Fields: {[f['name'] + ' (' + f['displayName'] + ')' for f in semantic_fields]}")

    # Build column mapping: Textract column index -> semantic field name
    column_mapping = {}

    # Check if headers are empty OR if we have duplicate headers (like multiple "Case Sales")
    headers_empty = all(not h or not h.strip() for h in headers)
    has_duplicates = len(headers) != len(set(h.strip().lower() for h in headers if h and h.strip()))

    if headers_empty or has_duplicates:
        if headers_empty:
            context.log.warning("âš ï¸  All headers are empty! Using positional mapping for NABCA table.")
        else:
            context.log.warning(f"âš ï¸  Duplicate headers detected! Using positional mapping.")
            context.log.info(f"    Headers: {headers}")

        # Use positional mapping for NABCA Brand Leaders table
        # Standard NABCA Brand Leaders columns are in this order
        if len(semantic_fields) == len(headers):
            for idx, field in enumerate(semantic_fields):
                column_mapping[idx] = field["name"]
                context.log.info(f"âœ… Positional mapping: column {idx} -> '{field['name']}' ({field['displayName']})")
        else:
            context.log.error(f"Column count mismatch: {len(headers)} table columns vs {len(semantic_fields)} entity fields")
    else:
        # Use fuzzy matching when headers exist
        for col_idx, header in enumerate(headers):
            if not header or not header.strip():
                continue

            # Find best matching semantic field
            best_match = None
            best_score = 0.0

            for field in semantic_fields:
                # Compare header with both field name and display name
                score1 = similarity(header.lower(), field["name"].lower())
                score2 = similarity(header.lower(), field["displayName"].lower())
                score = max(score1, score2)

                if score > best_score:
                    best_score = score
                    best_match = field["name"]

            # Only map if similarity is above threshold
            if best_match and best_score > 0.5:
                column_mapping[col_idx] = best_match
                context.log.info(f"âœ… Mapped column {col_idx} '{header}' -> '{best_match}' (score: {best_score:.2f})")
            else:
                context.log.info(f"âš ï¸  No good match for column '{header}' (best: '{best_match}' with score {best_score:.2f})")

    context.log.info(f"ðŸ“Š Column Mapping Complete: {len(column_mapping)} of {len(headers)} columns mapped")
    context.log.info(f"Mapping: {column_mapping}")

    # Extract records
    records = []
    skipped_header_count = 0
    for row in data_rows:
        # Skip rows that look like headers or summary rows
        if is_header_or_summary_row(row):
            skipped_header_count += 1
            context.log.debug(f"Skipping header/summary row: {row[:3]}...")
            continue

        record = {}
        for col_idx, value in enumerate(row):
            if col_idx in column_mapping:
                field_name = column_mapping[col_idx]
                record[field_name] = value.strip() if isinstance(value, str) else value

        # Only include records that have at least some mapped fields
        if len(record) >= len(semantic_fields) * 0.5:  # At least 50% of fields mapped
            records.append(record)
        else:
            context.log.warning(f"Skipping row with too few mapped fields: {record}")

    if skipped_header_count > 0:
        context.log.info(f"ðŸš« Skipped {skipped_header_count} header/summary rows")

    context.log.info(f"Extracted {len(records)} valid records")

    return records


def is_header_or_summary_row(row: List) -> bool:
    """
    Check if a row looks like a header or summary row that should be skipped.
    Returns True if the row contains common header/summary indicators.
    """
    # Keywords that indicate header or summary rows
    header_keywords = [
        'YEAR', 'LAST YEAR', 'THIS YEAR', 'PREVIOUS YEAR',
        'DECREASE', 'INCREASE', 'CHANGE', 'TOTAL', 'TOTALS',
        'CLASS', 'TYPE', 'CATEGORY', 'BRAND', 'RANK',
        'MONTH', 'MONTHS', 'YTD', 'ROLLING',
        'CASE SALES', 'SALES', 'VOLUME'
    ]

    # Check first few cells of the row
    for i, cell in enumerate(row[:3]):  # Check first 3 cells
        if not cell or not isinstance(cell, str):
            continue

        cell_upper = cell.strip().upper()

        # Skip if cell exactly matches a header keyword
        if cell_upper in header_keywords:
            return True

        # Skip if cell contains multiple header keywords (likely a composite header)
        keyword_count = sum(1 for keyword in header_keywords if keyword in cell_upper)
        if keyword_count >= 2:
            return True

    return False


def similarity(a: str, b: str) -> float:
    """Calculate similarity ratio between two strings."""
    return SequenceMatcher(None, a, b).ratio()


# ============================================================================
# TRANSFORMATION ASSETS
# ============================================================================

# No transformation needed for INTERIM entities
# Data flows directly to downstream Reference/Master entities


# ============================================================================
# LOAD ASSETS
# ============================================================================

@asset(
    name="load_raw_nabca_table_6",
    description="Load transformed data into raw_nabca_table_6 table",
    compute_kind="load",
    deps=["extract_raw_nabca_table_6"],
    retry_policy=RetryPolicy(max_retries=3),
)
def load_raw_nabca_table_6(
    context: AssetExecutionContext,
    extract_raw_nabca_table_6: Dict[str, Any]
) -> MaterializeResult:
    """
    Load raw_nabca_table_6 data into database.

    Target Table: raw_nabca_table_6
    Entity Type: INTERIM
    """
    try:
        context.log.info(f"Starting load for raw_nabca_table_6")

        # Initialize Supabase client
        from supabase import create_client
        import os

        supabase = create_client(
            os.getenv("NEXT_PUBLIC_SUPABASE_URL"),
            os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        )

        records = extract_raw_nabca_table_6["records"]

        if not records:
            context.log.warning("No records to load")
            return MaterializeResult(
                metadata={
                    "records_loaded": 0,
                    "records_failed": 0,
                }
            )

        # Remove metadata fields and clean data types
        clean_records = []
        for record in records:
            clean_record = {}
            for k, v in record.items():
                # Skip metadata fields
                if k.startswith('_'):
                    continue

                # Convert empty strings to None
                if v == '' or v is None:
                    clean_record[k] = None
                else:
                    # Remove commas and convert to proper types
                    if isinstance(v, str):
                        # Remove leading apostrophes (common in CSV/Textract output), percentages, commas, and spaces
                        v_clean = v.lstrip("'").replace('%', '').replace(',', '').replace(' ', '').strip()

                        # Try to convert to number if it looks numeric
                        if v_clean and v_clean.replace('.', '', 1).replace('-', '', 1).replace('+', '', 1).isdigit():
                            try:
                                clean_record[k] = float(v_clean) if '.' in v_clean else int(v_clean)
                            except ValueError:
                                clean_record[k] = v
                        else:
                            clean_record[k] = v
                    else:
                        clean_record[k] = v

            clean_records.append(clean_record)

        context.log.info(f"Cleaned {len(clean_records)} records (removed metadata, converted types)")

        # Batch insert with error handling
        batch_size = 100
        loaded_count = 0
        failed_count = 0

        for i in range(0, len(clean_records), batch_size):
            batch = clean_records[i:i + batch_size]

            try:
                response = supabase.table("raw_nabca_table_6").insert(batch).execute()
                loaded_count += len(batch)
                context.log.info(f"Loaded batch {i//batch_size + 1}: {len(batch)} records")

            except Exception as e:
                context.log.error(f"Batch insert failed: {str(e)}")

                # Try inserting records one by one
                for record in batch:
                    try:
                        supabase.table("raw_nabca_table_6").insert(record).execute()
                        loaded_count += 1
                    except Exception as record_error:
                        context.log.error(
                            f"Failed to insert record: {str(record_error)}"
                        )
                        failed_count += 1

        success_rate = (loaded_count / len(clean_records) * 100) if clean_records else 0

        context.log.info(
            f"Load complete: {loaded_count} loaded, {failed_count} failed "
            f"({success_rate:.1f}% success rate)"
        )

        # Quality check
        if success_rate < 95:
            context.log.warning(
                f"âš ï¸ Load success rate below 95%: {success_rate:.1f}%"
            )

        return MaterializeResult(
            metadata={
                "records_loaded": MetadataValue.int(loaded_count),
                "records_failed": MetadataValue.int(failed_count),
                "success_rate": MetadataValue.float(success_rate),
                "table_name": MetadataValue.text("raw_nabca_table_6"),
            }
        )

    except Exception as e:
        context.log.error(f"Load failed: {str(e)}")
        context.log.error(traceback.format_exc())
        raise


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def validate_record(record: Dict[str, Any], required_fields: List[str]) -> bool:
    """Validate that all required fields are present and non-empty."""
    for field in required_fields:
        if field not in record or record[field] is None or record[field] == "":
            return False
    return True


def deduplicate_records(
    records: List[Dict[str, Any]],
    key_fields: List[str]
) -> List[Dict[str, Any]]:
    """Remove duplicate records based on key fields."""
    seen = set()
    unique_records = []

    for record in records:
        # Create a key from specified fields
        key = tuple(record.get(field) for field in key_fields)

        if key not in seen:
            seen.add(key)
            unique_records.append(record)

    return unique_records


def parse_date(value: Any) -> Optional[str]:
    """Parse various date formats to ISO format."""
    if not value:
        return None

    import dateutil.parser

    try:
        if isinstance(value, str):
            dt = dateutil.parser.parse(value)
            return dt.date().isoformat()
        return str(value)
    except Exception:
        return None


def parse_timestamp(value: Any) -> Optional[str]:
    """Parse various timestamp formats to ISO format."""
    if not value:
        return None

    import dateutil.parser

    try:
        if isinstance(value, str):
            dt = dateutil.parser.parse(value)
            return dt.isoformat()
        return str(value)
    except Exception:
        return None

