/**
 * Pipeline Code Generator
 *
 * Generates Dagster pipeline code from entity models and templates
 */

interface Entity {
  id: string;
  name: string;
  display_name: string;
  entity_type: 'INTERIM' | 'REFERENCE' | 'MASTER';
  description: string | null;
}

interface EntityField {
  id: string;
  name: string;
  display_name: string;
  data_type: string;
  is_required: boolean;
  template_field_path?: string;
}

interface Template {
  id: string;
  name: string;
  fields: any;
  prompt: string;
}

interface Source {
  id: string;
  name: string;
  source_type: 'URL' | 'S3' | 'FILE_UPLOAD' | 'API';
  url?: string;
  s3_bucket?: string;
  s3_prefix?: string;
  s3_region?: string;
}

interface PipelineConfig {
  name: string;
  description?: string;
  entity: Entity;
  entityFields: EntityField[];
  template?: Template;
  sources: Source[];
  schedule?: string;
}

/**
 * Generate complete Dagster pipeline code
 */
export function generatePipelineCode(config: PipelineConfig): string {
  const {
    name,
    description,
    entity,
    entityFields,
    template,
    sources,
  } = config;

  const safeName = toSnakeCase(name);
  const entityTableName = entity.name;

  const code = `"""
${description || `Auto-generated pipeline for ${entity.display_name}`}

Generated by Inspector Dom
Entity: ${entity.display_name} (${entity.entity_type})
${template ? `Template: ${template.name}` : 'No template linked'}
"""

from dagster import asset, AssetExecutionContext, Output, MetadataValue
from typing import Dict, List, Any
import json
import os
from datetime import datetime
import boto3
from anthropic import Anthropic
import psycopg2
from psycopg2.extras import Json

# ============================================================================
# CONFIGURATION
# ============================================================================

SUPABASE_DB_URL = os.getenv("SUPABASE_DB_URL")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
AWS_ACCESS_KEY_ID = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")

# Entity configuration
ENTITY_NAME = "${entityTableName}"
ENTITY_TYPE = "${entity.entity_type}"

# Template configuration
${template ? `TEMPLATE_ID = "${template.id}"
TEMPLATE_FIELDS = ${JSON.stringify(template.fields, null, 2)}
EXTRACTION_PROMPT = """${template.prompt}"""` : '# No template configured'}

# Source configuration
SOURCES = ${JSON.stringify(sources.map(s => ({
  id: s.id,
  name: s.name,
  type: s.source_type,
  s3_bucket: s.s3_bucket,
  s3_prefix: s.s3_prefix,
  s3_region: s.s3_region || 'us-east-1',
  url: s.url,
})), null, 2)}

# Field mappings: template field -> entity column
FIELD_MAPPINGS = {
${entityFields
  .filter(f => f.template_field_path)
  .map(f => `    "${f.template_field_path}": "${f.name}"`)
  .join(',\n')}
}

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_db_connection():
    """Get PostgreSQL database connection"""
    return psycopg2.connect(SUPABASE_DB_URL)

def extract_with_ai(file_content: str, file_type: str) -> Dict[str, Any]:
    """Extract data using Claude AI"""
    client = Anthropic(api_key=ANTHROPIC_API_KEY)

    prompt = f"""Extract structured data from this {file_type} document.

{EXTRACTION_PROMPT if 'EXTRACTION_PROMPT' in dir() else 'Extract all relevant fields from the document.'}

Return the data as a JSON object with these fields:
{json.dumps(TEMPLATE_FIELDS if 'TEMPLATE_FIELDS' in dir() else [], indent=2)}

Document content:
{file_content[:50000]}  # Limit to 50k chars
"""

    message = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=4096,
        messages=[{"role": "user", "content": prompt}]
    )

    # Extract JSON from response
    content = message.content[0].text
    # Try to find JSON in response
    import re
    json_match = re.search(r'\\{[\\s\\S]*\\}', content)
    if json_match:
        return json.loads(json_match.group())
    return {}

def apply_template(file_content: str, file_type: str) -> Dict[str, Any]:
    """Apply template-based extraction (faster, cheaper)"""
    # TODO: Implement template-based extraction using selectors
    # For now, fall back to AI
    return extract_with_ai(file_content, file_type)

def transform_to_entity_row(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    """Transform extracted data to entity table structure"""
    row = {}

    # Map template fields to entity columns
    for template_path, entity_col in FIELD_MAPPINGS.items():
        # Extract nested field using path (e.g., "fields.TTB ID")
        value = raw_data
        for key in template_path.split('.'):
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                value = None
                break
        row[entity_col] = value

    # Add metadata fields
    row['extraction_date'] = datetime.now().isoformat()
    row['source_system'] = 'inspector_dom'

    return row

# ============================================================================
# DAGSTER ASSETS
# ============================================================================

@asset(
    name="${safeName}_fetch_files",
    description="Fetch files from configured sources",
    group_name="${safeName}"
)
def fetch_files(context: AssetExecutionContext) -> List[Dict[str, Any]]:
    """
    Fetch files from all configured sources.
    Returns list of file metadata with S3 paths or URLs.
    """
    all_files = []

    for source in SOURCES:
        context.log.info(f"Fetching files from source: {source['name']}")

        if source['type'] == 'S3':
            # Fetch from S3 bucket
            s3_client = boto3.client(
                's3',
                region_name=source['s3_region'],
                aws_access_key_id=AWS_ACCESS_KEY_ID,
                aws_secret_access_key=AWS_SECRET_ACCESS_KEY
            )

            response = s3_client.list_objects_v2(
                Bucket=source['s3_bucket'],
                Prefix=source.get('s3_prefix', '')
            )

            for obj in response.get('Contents', []):
                all_files.append({
                    'source_id': source['id'],
                    'source_name': source['name'],
                    'file_path': obj['Key'],
                    's3_bucket': source['s3_bucket'],
                    's3_region': source['s3_region'],
                    'file_size': obj['Size'],
                    'last_modified': obj['LastModified'].isoformat(),
                })

        elif source['type'] == 'URL':
            # Single URL source
            all_files.append({
                'source_id': source['id'],
                'source_name': source['name'],
                'url': source['url'],
            })

    context.log.info(f"Found {len(all_files)} files to process")

    return Output(
        value=all_files,
        metadata={
            "num_files": len(all_files),
            "sources": len(SOURCES),
        }
    )

@asset(
    name="${safeName}_extract_data",
    description="Extract data from files using template or AI",
    group_name="${safeName}"
)
def extract_data(
    context: AssetExecutionContext,
    ${safeName}_fetch_files: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Extract structured data from files.
    Tries template first (fast), falls back to AI if needed.
    """
    extracted_records = []
    template_successes = 0
    ai_fallbacks = 0
    errors = 0

    for file_info in ${safeName}_fetch_files:
        try:
            context.log.info(f"Processing: {file_info.get('file_path', file_info.get('url'))}")

            # Get file content
            if 's3_bucket' in file_info:
                s3_client = boto3.client('s3')
                response = s3_client.get_object(
                    Bucket=file_info['s3_bucket'],
                    Key=file_info['file_path']
                )
                file_content = response['Body'].read().decode('utf-8', errors='ignore')
                file_type = file_info['file_path'].split('.')[-1]
            else:
                # Fetch from URL
                import requests
                response = requests.get(file_info['url'])
                file_content = response.text
                file_type = 'html'

            # Try template-based extraction first
            try:
                extracted_data = apply_template(file_content, file_type)
                template_successes += 1
                extraction_method = 'template'
            except Exception as e:
                context.log.warning(f"Template extraction failed, using AI: {e}")
                extracted_data = extract_with_ai(file_content, file_type)
                ai_fallbacks += 1
                extraction_method = 'ai'

            extracted_records.append({
                **file_info,
                'extracted_data': extracted_data,
                'extraction_method': extraction_method,
                'extraction_timestamp': datetime.now().isoformat(),
            })

        except Exception as e:
            context.log.error(f"Error processing file: {e}")
            errors += 1

    context.log.info(
        f"Extraction complete: {template_successes} template, "
        f"{ai_fallbacks} AI fallback, {errors} errors"
    )

    return Output(
        value=extracted_records,
        metadata={
            "records_extracted": len(extracted_records),
            "template_successes": template_successes,
            "ai_fallbacks": ai_fallbacks,
            "errors": errors,
        }
    )

@asset(
    name="${safeName}_load_to_${entityTableName}",
    description=f"Load extracted data into {ENTITY_NAME} table",
    group_name="${safeName}"
)
def load_to_entity(
    context: AssetExecutionContext,
    ${safeName}_extract_data: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Transform and load extracted data into entity table.
    """
    conn = get_db_connection()
    cursor = conn.cursor()

    rows_inserted = 0
    rows_failed = 0

    try:
        for record in ${safeName}_extract_data:
            try:
                # Transform to entity structure
                entity_row = transform_to_entity_row(record['extracted_data'])

                # Build INSERT statement
                columns = list(entity_row.keys())
                values = [entity_row[col] for col in columns]
                placeholders = ', '.join(['%s'] * len(columns))

                insert_sql = f"""
                    INSERT INTO {ENTITY_NAME} ({', '.join(columns)})
                    VALUES ({placeholders})
                    ON CONFLICT DO NOTHING
                """

                cursor.execute(insert_sql, values)
                rows_inserted += 1

            except Exception as e:
                context.log.error(f"Error inserting row: {e}")
                rows_failed += 1

        conn.commit()

    finally:
        cursor.close()
        conn.close()

    context.log.info(f"Load complete: {rows_inserted} inserted, {rows_failed} failed")

    return Output(
        value={
            "rows_inserted": rows_inserted,
            "rows_failed": rows_failed,
            "entity_name": ENTITY_NAME,
        },
        metadata={
            "rows_inserted": rows_inserted,
            "rows_failed": rows_failed,
        }
    )
`;

  return code;
}

/**
 * Generate SQL to create entity table in data warehouse
 */
export function generateEntityTableSQL(
  entity: Entity,
  fields: EntityField[]
): string {
  const sqlType = (dataType: string): string => {
    switch (dataType) {
      case 'TEXT': return 'TEXT';
      case 'NUMBER': return 'NUMERIC';
      case 'DATE': return 'TIMESTAMPTZ';
      case 'BOOLEAN': return 'BOOLEAN';
      case 'JSON': return 'JSONB';
      case 'UUID': return 'UUID';
      default: return 'TEXT';
    }
  };

  const columns = fields.map(f => {
    const nullable = f.is_required ? 'NOT NULL' : '';
    // Quote column names to handle reserved words (to, date, from, etc.)
    return `  "${f.name}" ${sqlType(f.data_type)} ${nullable}`;
  });

  // Add metadata columns
  columns.push('  extraction_date TIMESTAMPTZ DEFAULT NOW()');
  columns.push('  source_system TEXT DEFAULT \'inspector_dom\'');
  columns.push('  created_at TIMESTAMPTZ DEFAULT NOW()');
  columns.push('  updated_at TIMESTAMPTZ DEFAULT NOW()');

  return `-- Entity table: ${entity.display_name}
-- Type: ${entity.entity_type}
${entity.description ? `-- ${entity.description}` : ''}

CREATE TABLE IF NOT EXISTS ${entity.name} (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
${columns.join(',\n')}
);

-- Indexes
CREATE INDEX IF NOT EXISTS idx_${entity.name}_extraction_date
  ON ${entity.name}(extraction_date DESC);
CREATE INDEX IF NOT EXISTS idx_${entity.name}_created_at
  ON ${entity.name}(created_at DESC);
`;
}

/**
 * Helper: Convert string to snake_case
 */
function toSnakeCase(str: string): string {
  return str
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, '_')
    .replace(/^_+|_+$/g, '');
}

/**
 * Generate deployment configuration for Dagster Cloud
 */
export function generateDagsterConfig(pipelineName: string, schedule?: string): string {
  const safeName = toSnakeCase(pipelineName);

  return `# Dagster workspace configuration
# Location: ${safeName}

locations:
  - location_name: ${safeName}
    code_source:
      python_file: ${safeName}.py

    # Schedule configuration
    ${schedule ? `schedules:
      - schedule_name: ${safeName}_schedule
        cron_schedule: "${schedule}"
        pipeline_name: ${safeName}` : '# No schedule configured'}
`;
}
